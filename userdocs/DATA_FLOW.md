# Data Flow: Input â†’ Validation â†’ Training

## Quick Answer to Your Questions

### 1. Where is the input source?

**Answer**: Input files are in the **project root directory** (same folder where you run `make validate-all`):

```
ğŸ“ projectwork/              â† You are here (project root)
â”œâ”€â”€ ğŸ“„ Event_traces.csv      â† INPUT (main data)
â”œâ”€â”€ ğŸ“„ anomaly_label.csv     â† INPUT (labels)
â”œâ”€â”€ ğŸ“„ HDFS.log_templates.csv â† INPUT (templates)
â””â”€â”€ ğŸ“„ HDFS.log              â† INPUT (optional, raw log)
```

These CSV files are your **INPUT SOURCES**. The validation pipeline reads them from this location.

### 2. Is there output for training, or just validation?

**Answer**: **Validation only** - no transformed data output.

- âœ… **Output**: Validation reports (PASS/FAIL, statistics)
- âŒ **NOT Output**: Transformed data files, training-ready datasets

**Data transformation happens in separate training scripts** (which you'll create later).

---

## Complete Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 1: INPUT FILES (Project Root)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“„ Event_traces.csv       (575k rows)                     â”‚
â”‚  ğŸ“„ anomaly_label.csv      (575k rows)                      â”‚
â”‚  ğŸ“„ HDFS.log_templates.csv (29 rows)                        â”‚
â”‚  ğŸ“„ HDFS.log               (optional, 1GB+)                â”‚
â”‚                                                              â”‚
â”‚  These are your INPUT SOURCES - the pipeline reads them     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 2: VALIDATION PIPELINE (This Tool)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Reads: Event_traces.csv, anomaly_label.csv, etc.          â”‚
â”‚  Checks: Format, quality, integrity, consistency             â”‚
â”‚  Outputs: Validation reports (NOT transformed data)        â”‚
â”‚                                                              â”‚
â”‚  ğŸ“Š artifacts/validation/validation_report.md              â”‚
â”‚  ğŸ“Š artifacts/validation/summary.json                       â”‚
â”‚  ğŸ“Š artifacts/validation/sequence_stats.json                â”‚
â”‚  ğŸ“Š ... (other validation reports)                          â”‚
â”‚                                                              â”‚
â”‚  âœ… PASS â†’ Data is ready for training                       â”‚
â”‚  âŒ FAIL â†’ Fix issues before training                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
              [Validation Status: PASS]
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 3: DATA TRANSFORMATION (Training Scripts)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Reads: Same Event_traces.csv, anomaly_label.csv            â”‚
â”‚  Processes:                                                 â”‚
â”‚    â€¢ Parse JSON columns (Features, TimeInterval)            â”‚
â”‚    â€¢ Create sliding windows (h=10)                          â”‚
â”‚    â€¢ Split by BlockID (80/10/10)                            â”‚
â”‚    â€¢ Convert to arrays/tensors                              â”‚
â”‚  Outputs: Training-ready data (in memory or files)         â”‚
â”‚                                                              â”‚
â”‚  ğŸ“ train_key.py    (creates windows from Features)        â”‚
â”‚  ğŸ“ train_value.py  (creates windows from TimeInterval)     â”‚
â”‚                                                              â”‚
â”‚  This is NOT part of the validation pipeline                â”‚
â”‚  You create these scripts separately                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 4: MODEL TRAINING                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input: Training-ready data from Phase 3                    â”‚
â”‚  Process: Train LSTM models (Key LSTM, Value LSTM)        â”‚
â”‚  Output: Trained model files (.pth, .h5, etc.)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## What the Validation Pipeline Does

### âœ… What It Does

1. **Reads** your input CSV files from project root
2. **Validates** format, structure, and data quality
3. **Checks** data integrity (BlockID consistency, label distribution, etc.)
4. **Generates** validation reports showing PASS/FAIL status
5. **Creates** documentation for future training scripts

### âŒ What It Does NOT Do

1. **Does NOT** transform data (no parsing, windowing, splitting)
2. **Does NOT** create training-ready files
3. **Does NOT** modify your input files
4. **Does NOT** output processed data

## Where to Find Transformation Code

The **data transformation** code (parsing, windowing, splitting) is documented in:

ğŸ“– **`report/DATA_PREPROCESSING_README.md`**

This file contains:
- Complete code examples for parsing JSON columns
- How to create sliding windows
- How to split data by BlockID
- How to prepare data for training

You'll use these examples to create your training scripts (`train_key.py`, `train_value.py`, etc.).

## Summary

| Question | Answer |
|---------|--------|
| **Where is input?** | Project root: `./Event_traces.csv`, `./anomaly_label.csv`, etc. |
| **What does validation output?** | Validation reports (PASS/FAIL), statistics, documentation |
| **Does it transform data?** | âŒ No - validation only |
| **Where does transformation happen?** | In separate training scripts (you create them) |
| **Where is transformation code?** | Examples in `report/DATA_PREPROCESSING_README.md` |

---

**Next Steps**: After validation passes, create training scripts using examples from `report/DATA_PREPROCESSING_README.md`.

